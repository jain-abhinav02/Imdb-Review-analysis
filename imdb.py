# -*- coding: utf-8 -*-
"""IMDB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nmmHqXXDKQZ0mSz4tZkL5Qe9KIOc9i4m
"""

!pip install numpy==1.16.1
import numpy as np

from keras.datasets import imdb
from keras.models import Sequential 
from keras.layers import LSTM,Embedding,Dense,Dropout
from keras.preprocessing import sequence

vocab_size=5000
(x_train,y_train),(x_test,y_test)=imdb.load_data(num_words=vocab_size)

#dictionary
word2id=imdb.get_word_index()
id2word={i:word for word,i in word2id.items()}

review=[id2word.get(i,'') for i in x_train[10]]
print(review)

#calculate max/min length of any review
mx=len(max(x_train+x_test,key=len))
mn=len(min(x_train+x_test,key=len))
#padding
maxlen=50
x_train1=sequence.pad_sequences(x_train, maxlen)
x_test1=sequence.pad_sequences(x_test,maxlen)

#split dataset into train and validation set
valid_size=64
x_train2=x_train1[valid_size:]
x_val=x_train1[:valid_size]
y_train2=y_train[valid_size:]
y_valid=y_train[:valid_size]

#model
embedding_size=32
model=Sequential()
model.add(Embedding(vocab_size,embedding_size,input_length=maxlen))
model.add(LSTM(100))
model.add(Dense(1,activation='sigmoid'))

print(model.summary())
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

checkpoint_path = "training_1/cp.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)

# Create checkpoint callback
cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,
                                                 save_weights_only=True,
                                                 verbose=1)

num_epochs=1
batch_size=32
model.fit(x_train2,y_train2,validation_data=(x_val,y_valid),batch_size=batch_size,epochs=num_epochs,callbacks=[cp_callback])

scores=model.evaluate(x_test1,y_test,verbose=1)
print("Acurracy= "+str(scores[1]))

x_runtime=np.array(['it','was','a','very','bad','movie','the','story','was','awful','and','the' ,'acting','was' ,'pathetic'],dtype='object')

print(x_runtime)

x_runtime=np.array([word2id.get(word,0) for word in x_runtime],dtype='object')
print(x_runtime)

x_runtime=x_runtime.reshape(1,x_runtime.shape[0])
x_runtime=sequence.pad_sequences(x_runtime,maxlen)

print(model.predict(x_runtime))

print(x_train.dtype)